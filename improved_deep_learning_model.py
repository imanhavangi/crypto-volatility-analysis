import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import Sequence
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ØªÙ†Ø¸ÛŒÙ… GPU Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    tf.config.experimental.set_memory_growth(gpus[0], True)

class FocalLoss(tf.keras.losses.Loss):
    """
    Focal Loss Ø¨Ø±Ø§ÛŒ Ø­Ù„ Ù…Ø´Ú©Ù„ class imbalance
    """
    def __init__(self, alpha=0.25, gamma=2.0, **kwargs):
        super().__init__(**kwargs)
        self.alpha = alpha
        self.gamma = gamma

    def call(self, y_true, y_pred):
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)
        
        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
        alpha_t = tf.where(tf.equal(y_true, 1), self.alpha, 1 - self.alpha)
        
        focal_loss = -alpha_t * tf.math.pow(1 - pt, self.gamma) * tf.math.log(pt)
        return tf.reduce_mean(focal_loss)

class ImprovedTradingModel:
    def __init__(self, input_features=20, use_focal_loss=True):
        """
        Ù…Ø¯Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØªØ±ÛŒØ¯ Ø¨Ø§ Ø­Ù„ Ù…Ø´Ú©Ù„ class imbalance
        
        Args:
            input_features: ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ
            use_focal_loss: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Focal Loss
        """
        self.input_features = input_features
        self.use_focal_loss = use_focal_loss
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = None
        self.class_weights_entry = None
        self.class_weights_exit = None
        self.smote = SMOTE(random_state=42, k_neighbors=3)
        self.threshold_entry = 0.5
        self.threshold_exit = 0.5
        
    def calculate_class_weights(self, y_entry, y_exit):
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³ Ø¨Ø± Ø§Ø³Ø§Ø³ inverse frequency
        """
        from sklearn.utils.class_weight import compute_class_weight
        
        # Entry weights
        entry_classes = np.unique(y_entry)
        entry_weights = compute_class_weight('balanced', classes=entry_classes, y=y_entry)
        self.class_weights_entry = dict(zip(entry_classes, entry_weights))
        
        # Exit weights  
        exit_classes = np.unique(y_exit)
        exit_weights = compute_class_weight('balanced', classes=exit_classes, y=y_exit)
        self.class_weights_exit = dict(zip(exit_classes, exit_weights))
        
        print(f"ğŸ† Entry Class Weights: {self.class_weights_entry}")
        print(f"ğŸ† Exit Class Weights: {self.class_weights_exit}")
        
    def create_model_architecture(self):
        """
        Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¨Ø§ attention mechanism
        """
        # Input layer
        inputs = layers.Input(shape=(self.input_features,), name='features')
        
        # Feature engineering layers
        x = layers.Dense(256, activation='relu', name='feature_extract_1')(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        
        x = layers.Dense(128, activation='relu', name='feature_extract_2')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        # Attention mechanism
        attention = layers.Dense(128, activation='tanh', name='attention_weights')(x)
        attention = layers.Dense(1, activation='sigmoid', name='attention_scores')(attention)
        x_attended = layers.Multiply(name='attended_features')([x, attention])
        
        # Shared representation
        shared = layers.Dense(64, activation='relu', name='shared_representation')(x_attended)
        shared = layers.Dropout(0.1)(shared)
        
        # Entry prediction branch
        entry_branch = layers.Dense(32, activation='relu', name='entry_branch')(shared)
        entry_branch = layers.Dropout(0.1)(entry_branch)
        entry_output = layers.Dense(1, activation='sigmoid', name='entry_prediction')(entry_branch)
        
        # Exit prediction branch
        exit_branch = layers.Dense(32, activation='relu', name='exit_branch')(shared)
        exit_branch = layers.Dropout(0.1)(exit_branch)
        exit_output = layers.Dense(1, activation='sigmoid', name='exit_prediction')(exit_branch)
        
        # Create model
        model = keras.Model(inputs=inputs, outputs=[entry_output, exit_output])
        
        return model
    
    def compile_model(self):
        """
        Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ù…Ø¯Ù„ Ø¨Ø§ loss functions Ùˆ metrics Ù…Ù†Ø§Ø³Ø¨
        """
        if self.use_focal_loss:
            entry_loss = FocalLoss(alpha=0.75, gamma=2.0)  # alpha Ø¨Ø§Ù„Ø§ØªØ± Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³ Ø§Ù‚Ù„ÛŒØª
            exit_loss = FocalLoss(alpha=0.75, gamma=2.0)
        else:
            entry_loss = 'binary_crossentropy'
            exit_loss = 'binary_crossentropy'
            
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss={
                'entry_prediction': entry_loss,
                'exit_prediction': exit_loss
            },
            loss_weights={
                'entry_prediction': 1.0,
                'exit_prediction': 1.0
            },
            metrics={
                'entry_prediction': [
                    'accuracy',
                    tf.keras.metrics.Precision(name='precision'),
                    tf.keras.metrics.Recall(name='recall'),
                    tf.keras.metrics.AUC(name='auc'),
                    tf.keras.metrics.AUC(curve='PR', name='pr_auc')
                ],
                'exit_prediction': [
                    'accuracy', 
                    tf.keras.metrics.Precision(name='precision'),
                    tf.keras.metrics.Recall(name='recall'),
                    tf.keras.metrics.AUC(name='auc'),
                    tf.keras.metrics.AUC(curve='PR', name='pr_auc')
                ]
            }
        )
        
    def prepare_data(self, df):
        """
        Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ feature engineering Ùˆ SMOTE
        """
        print("ğŸ”§ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
        
        # Ø§Ù†ØªØ®Ø§Ø¨ ÙÛŒÚ†Ø±Ù‡Ø§
        feature_cols = [col for col in df.columns if col not in 
                       ['timestamp', 'is_optimal_entry', 'is_optimal_exit', 'future_profit_potential']]
        
        self.feature_columns = feature_cols
        X = df[feature_cols].values
        y_entry = df['is_optimal_entry'].values.astype(int)
        y_exit = df['is_optimal_exit'].values.astype(int)
        
        print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÚ†Ø±Ù‡Ø§: {len(feature_cols)}")
        print(f"ğŸ“Š Ø´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {X.shape}")
        
        # Ø§ÛŒØ¬Ø§Ø¯ target ØªØ±Ú©ÛŒØ¨ÛŒ Ø¨Ø±Ø§ÛŒ SMOTE
        y_combined = y_entry * 2 + y_exit  # 0: (0,0), 1: (0,1), 2: (1,0), 3: (1,1)
        
        print(f"ğŸ“Š ØªÙˆØ²ÛŒØ¹ ØªØ±Ú©ÛŒØ¨ÛŒ Ù‚Ø¨Ù„ Ø§Ø² SMOTE: {np.bincount(y_combined)}")
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Stratified Split
        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
        train_idx, test_idx = next(sss.split(X, y_combined))
        
        X_train, X_test = X[train_idx], X[test_idx]
        y_entry_train, y_entry_test = y_entry[train_idx], y_entry[test_idx]
        y_exit_train, y_exit_test = y_exit[train_idx], y_exit[test_idx]
        y_combined_train = y_combined[train_idx]
        
        # Scaling
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # SMOTE application
        print("ğŸ¯ Ø§Ø¹Ù…Ø§Ù„ SMOTE...")
        try:
            X_train_balanced, y_combined_balanced = self.smote.fit_resample(X_train_scaled, y_combined_train)
            
            # ØªØ¬Ø²ÛŒÙ‡ Ù…Ø¬Ø¯Ø¯ y_combined
            y_entry_balanced = (y_combined_balanced >= 2).astype(int)
            y_exit_balanced = (y_combined_balanced % 2).astype(int)
            
            print(f"ğŸ“Š ØªÙˆØ²ÛŒØ¹ Entry Ø¨Ø¹Ø¯ Ø§Ø² SMOTE: {np.bincount(y_entry_balanced)}")
            print(f"ğŸ“Š ØªÙˆØ²ÛŒØ¹ Exit Ø¨Ø¹Ø¯ Ø§Ø² SMOTE: {np.bincount(y_exit_balanced)}")
            
        except Exception as e:
            print(f"âš ï¸  SMOTE Ù†Ø§Ù…ÙˆÙÙ‚: {e}")
            print("ğŸ”„ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯ÙˆÙ† SMOTE...")
            X_train_balanced = X_train_scaled
            y_entry_balanced = y_entry_train
            y_exit_balanced = y_exit_train
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ class weights
        self.calculate_class_weights(y_entry_balanced, y_exit_balanced)
        
        return {
            'X_train': X_train_balanced,
            'X_test': X_test_scaled,
            'y_entry_train': y_entry_balanced,
            'y_entry_test': y_entry_test,
            'y_exit_train': y_exit_balanced,
            'y_exit_test': y_exit_test
        }
    
    def find_optimal_threshold(self, X_val, y_entry_val, y_exit_val):
        """
        ÛŒØ§ÙØªÙ† Ø¨Ù‡ØªØ±ÛŒÙ† threshold Ø¨Ø± Ø§Ø³Ø§Ø³ F1-score
        """
        print("ğŸ¯ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† threshold...")
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
        entry_probs, exit_probs = self.model.predict(X_val, verbose=0)
        
        thresholds = np.arange(0.1, 0.9, 0.05)
        best_entry_threshold = 0.5
        best_exit_threshold = 0.5
        best_entry_f1 = 0
        best_exit_f1 = 0
        
        # Entry threshold optimization
        for threshold in thresholds:
            entry_pred = (entry_probs > threshold).astype(int)
            f1 = f1_score(y_entry_val, entry_pred)
            if f1 > best_entry_f1:
                best_entry_f1 = f1
                best_entry_threshold = threshold
        
        # Exit threshold optimization  
        for threshold in thresholds:
            exit_pred = (exit_probs > threshold).astype(int)
            f1 = f1_score(y_exit_val, exit_pred)
            if f1 > best_exit_f1:
                best_exit_f1 = f1
                best_exit_threshold = threshold
                
        self.threshold_entry = best_entry_threshold
        self.threshold_exit = best_exit_threshold
        
        print(f"âœ… Ø¨Ù‡ØªØ±ÛŒÙ† Entry Threshold: {best_entry_threshold:.3f} (F1: {best_entry_f1:.3f})")
        print(f"âœ… Ø¨Ù‡ØªØ±ÛŒÙ† Exit Threshold: {best_exit_threshold:.3f} (F1: {best_exit_f1:.3f})")
    
    def train(self, training_file='training_data.csv', epochs=50, batch_size=256):
        """
        Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø§ ØªÙ…Ø§Ù… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§
        """
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡...")
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        df = pd.read_csv(training_file)
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        data = self.prepare_data(df)
        
        # ØªÙ†Ø¸ÛŒÙ… ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÚ†Ø±Ù‡Ø§
        self.input_features = len(self.feature_columns)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„
        self.model = self.create_model_architecture()
        self.compile_model()
        
        print("\nğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù…Ø¯Ù„:")
        self.model.summary()
        
        # Callbacks
        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor='val_loss', patience=10, restore_best_weights=True, verbose=1
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1
            ),
            keras.callbacks.ModelCheckpoint(
                'best_improved_model.h5', save_best_only=True, monitor='val_loss', verbose=1
            )
        ]
        
        # Ø§ÛŒØ¬Ø§Ø¯ validation split
        val_split = 0.15
        val_size = int(len(data['X_train']) * val_split)
        indices = np.random.permutation(len(data['X_train']))
        
        train_indices = indices[val_size:]
        val_indices = indices[:val_size]
        
        X_train_final = data['X_train'][train_indices]
        X_val = data['X_train'][val_indices]
        y_entry_train_final = data['y_entry_train'][train_indices]
        y_entry_val = data['y_entry_train'][val_indices]
        y_exit_train_final = data['y_exit_train'][train_indices]
        y_exit_val = data['y_exit_train'][val_indices]
        
        # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
        print("\nğŸ“ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´...")
        history = self.model.fit(
            X_train_final,
            {
                'entry_prediction': y_entry_train_final,
                'exit_prediction': y_exit_train_final
            },
            validation_data=(
                X_val,
                {
                    'entry_prediction': y_entry_val,
                    'exit_prediction': y_exit_val
                }
            ),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        # ÛŒØ§ÙØªÙ† Ø¨Ù‡ØªØ±ÛŒÙ† threshold
        self.find_optimal_threshold(X_val, y_entry_val, y_exit_val)
        
        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        self.evaluate_model(data['X_test'], data['y_entry_test'], data['y_exit_test'])
        
        # Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
        self.plot_training_history(history)
        
        return history
    
    def evaluate_model(self, X_test, y_entry_test, y_exit_test):
        """
        Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¯Ù‚ÛŒÙ‚ Ù…Ø¯Ù„ Ø¨Ø§ metrics Ù…Ù†Ø§Ø³Ø¨
        """
        print("\nğŸ“ˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø¯Ù„...")
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
        entry_probs, exit_probs = self.model.predict(X_test, verbose=0)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ threshold Ø¨Ù‡ÛŒÙ†Ù‡
        entry_pred = (entry_probs > self.threshold_entry).astype(int).flatten()
        exit_pred = (exit_probs > self.threshold_exit).astype(int).flatten()
        
        print("\nğŸ¯ Ù†ØªØ§ÛŒØ¬ Entry Prediction:")
        print(classification_report(y_entry_test, entry_pred, target_names=['No Entry', 'Entry']))
        
        print("\nğŸ¯ Ù†ØªØ§ÛŒØ¬ Exit Prediction:")
        print(classification_report(y_exit_test, exit_pred, target_names=['No Exit', 'Exit']))
        
        # AUC scores
        try:
            entry_auc = roc_auc_score(y_entry_test, entry_probs)
            exit_auc = roc_auc_score(y_exit_test, exit_probs)
            print(f"\nğŸ† Entry AUC-ROC: {entry_auc:.4f}")
            print(f"ğŸ† Exit AUC-ROC: {exit_auc:.4f}")
        except:
            print("âš ï¸ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† AUC Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø±Ø¯ (Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ ÙÙ‚Ø· ÛŒÚ© Ú©Ù„Ø§Ø³ Ø¯Ø± test set)")
        
        # Confusion matrices
        self.plot_confusion_matrices(y_entry_test, entry_pred, y_exit_test, exit_pred)
        
    def plot_confusion_matrices(self, y_entry_true, y_entry_pred, y_exit_true, y_exit_pred):
        """
        Ø±Ø³Ù… confusion matrices
        """
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        cm_entry = confusion_matrix(y_entry_true, y_entry_pred)
        sns.heatmap(cm_entry, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['No Entry', 'Entry'],
                   yticklabels=['No Entry', 'Entry'])
        plt.title('Entry Prediction Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        
        plt.subplot(1, 2, 2)
        cm_exit = confusion_matrix(y_exit_true, y_exit_pred)
        sns.heatmap(cm_exit, annot=True, fmt='d', cmap='Greens',
                   xticklabels=['No Exit', 'Exit'],
                   yticklabels=['No Exit', 'Exit'])
        plt.title('Exit Prediction Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        
        plt.tight_layout()
        plt.savefig('improved_model_confusion_matrices.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_training_history(self, history):
        """
        Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
        """
        plt.figure(figsize=(15, 10))
        
        # Loss plots
        plt.subplot(2, 3, 1)
        plt.plot(history.history['loss'], label='Train Loss')
        plt.plot(history.history['val_loss'], label='Val Loss')
        plt.title('Model Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        
        # Entry metrics
        plt.subplot(2, 3, 2)
        plt.plot(history.history['entry_prediction_precision'], label='Train Precision')
        plt.plot(history.history['val_entry_prediction_precision'], label='Val Precision')
        plt.plot(history.history['entry_prediction_recall'], label='Train Recall')
        plt.plot(history.history['val_entry_prediction_recall'], label='Val Recall')
        plt.title('Entry Prediction Metrics')
        plt.xlabel('Epoch')
        plt.ylabel('Score')
        plt.legend()
        
        # Exit metrics
        plt.subplot(2, 3, 3)
        plt.plot(history.history['exit_prediction_precision'], label='Train Precision')
        plt.plot(history.history['val_exit_prediction_precision'], label='Val Precision')
        plt.plot(history.history['exit_prediction_recall'], label='Train Recall')
        plt.plot(history.history['val_exit_prediction_recall'], label='Val Recall')
        plt.title('Exit Prediction Metrics')
        plt.xlabel('Epoch')
        plt.ylabel('Score')
        plt.legend()
        
        # AUC plots
        plt.subplot(2, 3, 4)
        plt.plot(history.history['entry_prediction_auc'], label='Entry AUC')
        plt.plot(history.history['val_entry_prediction_auc'], label='Val Entry AUC')
        plt.plot(history.history['exit_prediction_auc'], label='Exit AUC')
        plt.plot(history.history['val_exit_prediction_auc'], label='Val Exit AUC')
        plt.title('AUC Scores')
        plt.xlabel('Epoch')
        plt.ylabel('AUC')
        plt.legend()
        
        # PR AUC plots
        plt.subplot(2, 3, 5)
        plt.plot(history.history['entry_prediction_pr_auc'], label='Entry PR-AUC')
        plt.plot(history.history['val_entry_prediction_pr_auc'], label='Val Entry PR-AUC')
        plt.plot(history.history['exit_prediction_pr_auc'], label='Exit PR-AUC')
        plt.plot(history.history['val_exit_prediction_pr_auc'], label='Val Exit PR-AUC')
        plt.title('Precision-Recall AUC')
        plt.xlabel('Epoch')
        plt.ylabel('PR-AUC')
        plt.legend()
        
        # Learning rate
        plt.subplot(2, 3, 6)
        if 'lr' in history.history:
            plt.plot(history.history['lr'])
            plt.title('Learning Rate')
            plt.xlabel('Epoch')
            plt.ylabel('Learning Rate')
            plt.yscale('log')
        
        plt.tight_layout()
        plt.savefig('improved_model_training_history.png', dpi=300, bbox_inches='tight')
        plt.show()

if __name__ == "__main__":
    # Ø§ÛŒØ¬Ø§Ø¯ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
    model = ImprovedTradingModel(use_focal_loss=True)
    
    print("ğŸ¯ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø§ Ø­Ù„ Ù…Ø´Ú©Ù„ Class Imbalance")
    print("="*60)
    
    history = model.train(
        training_file='training_data.csv',
        epochs=2,
        batch_size=512
    )
    
    print("\nâœ… Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
    print(f"ğŸ¯ Entry Threshold: {model.threshold_entry:.3f}")
    print(f"ğŸ¯ Exit Threshold: {model.threshold_exit:.3f}") 